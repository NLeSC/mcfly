{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial PAMAP2 with mcfly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This turorial is intended to talk you through the functionalities of mcfly. As an example dataset we use the publicly available [PAMAP2 dataset](https://archive.ics.uci.edu/ml/datasets/PAMAP2+Physical+Activity+Monitoring). It contains time series data from movement sensors worn by nine individuals. The data is labelled with the activity types that these individuals did and the aim is to train and evaluate a *classifier*.\n",
    "\n",
    "Before you can start, please make sure you installed all the dependencies of mcfly (listed in requirements.txt) and make sure your jupyter notebook has a python3 kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# mcfly\n",
    "from mcfly import tutorial_pamap2, modelgen, find_architecture, storage\n",
    "# Keras module is use for the deep learning\n",
    "import keras\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Convolution1D, Flatten, MaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.abspath('../..'))\n",
    "import tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data and pre-proces data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a function for you to fetch and pre-proces the data. Please specify the 'directory_to_extract_to' in the code below and then execute the cell. This will download the data into the directory and create a subdirectory 'PAMAP2'. The output of the function is outputpath which indicates where the data was stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specify in which directory you want to store the data:\n",
    "directory_to_extract_to = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Name of the output directory with data\n",
    "outputdir = 'output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data previously downloaded and stored in data/PAMAP2/\n",
      "Start pre-processing all 9 files...\n",
      "Stored data/PAMAP2/PAMAP2_Dataset/output/X_train y_train\n",
      "Stored data/PAMAP2/PAMAP2_Dataset/output/X_val y_val\n",
      "Stored data/PAMAP2/PAMAP2_Dataset/output/X_test y_test\n",
      "Processed data succesfully stored in data/PAMAP2/PAMAP2_Dataset/output\n"
     ]
    }
   ],
   "source": [
    "# Specifcy which columns to use. You can leave this as it is \n",
    "columns_to_use = ['hand_acc_16g_x', 'hand_acc_16g_y', 'hand_acc_16g_z',\n",
    "                 'ankle_acc_16g_x', 'ankle_acc_16g_y', 'ankle_acc_16g_z',\n",
    "                 'chest_acc_16g_x', 'chest_acc_16g_y', 'chest_acc_16g_z']\n",
    "exclude_activities = [9, 10, 11, 18, 19, 20, 0]\n",
    "outputpath = tutorial.fetch_and_preprocess(directory_to_extract_to,columns_to_use, \n",
    "                                                  exclude_activities=exclude_activities,\n",
    "                                                 output_dir=outputdir,\n",
    "                                 val_test_size=(100, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the pre-processed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the preprocessed data as stored in Numpy-files. Please note that the data has already been split up in a training (training), validation (val), and test subsets. It is common practice to call the input data X and the labels y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train_binary, X_val, y_val_binary, X_test, y_test_binary = tutorial_pamap2.load_data(outputpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the shape of the data. The shape of X is a tuple of the number of samples, length of the time series, and the number of channels for each sample. The shape of y is a tuple of the number of samples and the number of classes. Labels are formatted as a binary array where only the correct class for each sample is assigned a 1. This is called one-hot-encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (15718, 512, 9)\n",
      "y shape: (15718, 12)\n"
     ]
    }
   ],
   "source": [
    "print('x shape:', X_train.shape)\n",
    "print('y shape:', y_train_binary.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is split between train test and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 15718\n",
      "validation set size: 100\n",
      "test set size: 1000\n"
     ]
    }
   ],
   "source": [
    "print('train set size:', X_train.shape[0])\n",
    "print('validation set size:', X_val.shape[0])\n",
    "print('test set size:', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to create a model architecture. As we do not know what architecture is best for our data we will create a set of models to investigate which architecture is most suitable for our data and classification task. You will need to specificy how many models you want to create with argument 'number_of_models'. See for a full overview of the optional arguments the function documentation of modelgen.generate_models by running `modelgen.generate_models?`.\n",
    "\n",
    "##### What number of models to select?\n",
    "This number differs per dataset. More models will give better results but it will take longer to evaluate them. For the purpose of this tutorial we recommend trying only 2 models to begin with. If you have enough time you can try a larger number of models. Because mcfly uses random search, you will get better results when using more models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dafne/anaconda2/envs/mcfly/lib/python3.5/site-packages/keras/engine/topology.py:368: UserWarning: The `regularizers` property of layers/models is deprecated. Regularization losses are now managed via the `losses` layer/model property.\n",
      "  warnings.warn('The `regularizers` property of '\n"
     ]
    }
   ],
   "source": [
    "num_classes = y_train_binary.shape[1]\n",
    "#%pdb on\n",
    "models = modelgen.generate_models(X_train.shape,\n",
    "                                  number_of_classes=num_classes,\n",
    "                                  number_of_models = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the models\n",
    "We can have a look at the models that were generated. The layers are shown as table rows. Most common layer types are 'Convolution' and 'LSTM' and 'Dense'. For more information see the [mcfly user manual](https://github.com/NLeSC/mcfly/wiki/User-manual). The summary also shows the data shape of each layer output and the number of parameters that are trained within this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------------\n",
      "Model 0\n",
      " \n",
      "Hyperparameters:\n",
      "{'regularization_rate': 0.000871406843381289, 'learning_rate': 0.05241610537437882, 'filters': array([76, 19, 18, 96]), 'fc_hidden_nodes': 1391}\n",
      " \n",
      "Model description:\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "batchnormalization_1 (BatchNorma (None, 512, 9)        36          batchnormalization_input_1[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_1 (Convolution1D)  (None, 512, 76)       2128        batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_2 (BatchNorma (None, 512, 76)       304         convolution1d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 512, 76)       0           batchnormalization_2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_2 (Convolution1D)  (None, 512, 19)       4351        activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_3 (BatchNorma (None, 512, 19)       76          convolution1d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 512, 19)       0           batchnormalization_3[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_3 (Convolution1D)  (None, 512, 18)       1044        activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_4 (BatchNorma (None, 512, 18)       72          convolution1d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 512, 18)       0           batchnormalization_4[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_4 (Convolution1D)  (None, 512, 96)       5280        activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_5 (BatchNorma (None, 512, 96)       384         convolution1d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 512, 96)       0           batchnormalization_5[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 49152)         0           activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1391)          68371823    flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 1391)          0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 12)            16704       activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_6 (BatchNorma (None, 12)            48          dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 12)            0           batchnormalization_6[0][0]       \n",
      "====================================================================================================\n",
      "Total params: 68,402,250\n",
      "Trainable params: 68,401,790\n",
      "Non-trainable params: 460\n",
      "____________________________________________________________________________________________________\n",
      " \n",
      "Model type:\n",
      "CNN\n",
      " \n",
      "-------------------------------------------------------------------------------------------------------\n",
      "Model 1\n",
      " \n",
      "Hyperparameters:\n",
      "{'regularization_rate': 0.0006385919929132891, 'learning_rate': 0.0013206889428859167, 'lstm_dims': array([47]), 'filters': array([68, 44, 31, 62, 67])}\n",
      " \n",
      "Model description:\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "batchnormalization_7 (BatchNorma (None, 512, 9)        36          batchnormalization_input_2[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)              (None, 1, 512, 9)     0           batchnormalization_7[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 68, 512, 9)    272         reshape_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_8 (BatchNorma (None, 68, 512, 9)    36          convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 68, 512, 9)    0           batchnormalization_8[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 44, 512, 9)    9020        activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_9 (BatchNorma (None, 44, 512, 9)    36          convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 44, 512, 9)    0           batchnormalization_9[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 31, 512, 9)    4123        activation_8[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_10 (BatchNorm (None, 31, 512, 9)    36          convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, 31, 512, 9)    0           batchnormalization_10[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 62, 512, 9)    5828        activation_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_11 (BatchNorm (None, 62, 512, 9)    36          convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 62, 512, 9)    0           batchnormalization_11[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 67, 512, 9)    12529       activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_12 (BatchNorm (None, 67, 512, 9)    36          convolution2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 67, 512, 9)    0           batchnormalization_12[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)              (None, 512, 603)      0           activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 512, 47)       122388      reshape_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 512, 47)       0           lstm_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_1 (TimeDistribut (None, 512, 12)       576         dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 512, 12)       0           timedistributed_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)                (None, 12)            0           activation_12[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 154,952\n",
      "Trainable params: 154,844\n",
      "Non-trainable params: 108\n",
      "____________________________________________________________________________________________________\n",
      " \n",
      "Model type:\n",
      "DeepConvLSTM\n",
      " \n"
     ]
    }
   ],
   "source": [
    "models_to_print = range(len(models))\n",
    "for i, item in enumerate(models):\n",
    "    if i in models_to_print:\n",
    "        model, params, model_types = item\n",
    "        print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "        print(\"Model \" + str(i))\n",
    "        print(\" \")\n",
    "        print(\"Hyperparameters:\")\n",
    "        print(params)\n",
    "        print(\" \")\n",
    "        print(\"Model description:\")\n",
    "        model.summary()\n",
    "        print(\" \")\n",
    "        print(\"Model type:\")\n",
    "        print(model_types)\n",
    "        print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare models\n",
    "Now that the model architectures have been generated it is time to compare the models by training them in a subset of the training data and evaluating the models in the validation subset. This will help us to choose the best candidate model. Performance results are stored in a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define directory where the results, e.g. json file, will be stored\n",
    "\n",
    "resultpath = os.path.join(outputpath, 'models')\n",
    "if not os.path.exists(resultpath):\n",
    "        os.makedirs(resultpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to train each of the models that we generated. On the one hand we want to train them as quickly as possible in order to be able to pick the best one as soon as possible. On the other hand we have to train each model long enough to get a good impression of its potential.\n",
    "\n",
    "We can influence the train time by adjusting the number of data samples that are used. This can be set with the argument 'subset_size'. We can also adjust the number of times the subsample is looped over. This is called an epoch. We recommend to start with no more than 5 epochs and a maximum subset of 300. You can experiment with these numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 0 CNN\n",
      "Train on 300 samples, validate on 100 samples\n",
      "Epoch 1/5\n",
      "300/300 [==============================] - 79s - loss: 1337.7953 - acc: 0.3067 - val_loss: 2123.1167 - val_acc: 0.2400\n",
      "Epoch 2/5\n",
      "300/300 [==============================] - 135s - loss: 1785.7541 - acc: 0.3567 - val_loss: 1301.1616 - val_acc: 0.1800\n",
      "Epoch 3/5\n",
      "300/300 [==============================] - 137s - loss: 925.9993 - acc: 0.3367 - val_loss: 601.4759 - val_acc: 0.1900\n",
      "Epoch 4/5\n",
      "300/300 [==============================] - 82s - loss: 415.7455 - acc: 0.3667 - val_loss: 274.5558 - val_acc: 0.1900\n",
      "Epoch 5/5\n",
      "300/300 [==============================] - 73s - loss: 189.7508 - acc: 0.3600 - val_loss: 133.4206 - val_acc: 0.2000\n",
      "Training model 1 DeepConvLSTM\n",
      "Train on 300 samples, validate on 100 samples\n",
      "Epoch 1/5\n",
      "300/300 [==============================] - 242s - loss: 2.7215 - acc: 0.1500 - val_loss: 2.5231 - val_acc: 0.1500\n",
      "Epoch 2/5\n",
      "300/300 [==============================] - 241s - loss: 2.4704 - acc: 0.2100 - val_loss: 2.4666 - val_acc: 0.2800\n",
      "Epoch 3/5\n",
      "300/300 [==============================] - 235s - loss: 2.3242 - acc: 0.2867 - val_loss: 2.4112 - val_acc: 0.3600\n",
      "Epoch 4/5\n",
      "300/300 [==============================] - 238s - loss: 2.1173 - acc: 0.3833 - val_loss: 2.2983 - val_acc: 0.3500\n",
      "Epoch 5/5\n",
      "300/300 [==============================] - 235s - loss: 1.9356 - acc: 0.4267 - val_loss: 2.1144 - val_acc: 0.4300\n",
      "Details of the training process were stored in  data/PAMAP2/PAMAP2_Dataset/output/models/modelcomparison.json\n"
     ]
    }
   ],
   "source": [
    "outputfile = os.path.join(resultpath, 'modelcomparison.json')\n",
    "histories, val_accuracies, val_losses = find_architecture.train_models_on_samples(X_train, y_train_binary,\n",
    "                                                                           X_val, y_val_binary,\n",
    "                                                                           models,nr_epochs=5,\n",
    "                                                                           subset_size=300,\n",
    "                                                                           verbose=True,\n",
    "                                                                           outputfile=outputfile)\n",
    "print('Details of the training process were stored in ',outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect model performance (Visualization)\n",
    "\n",
    "Details about the learning process can be visualized. To use mcfly's visualization, navigate to the html folder and start a web server. For example `python3 -m http.server`.\n",
    "Notice the port number the web server is serving on. This is usually 8000.\n",
    "With a web browser, navigate to [localhost:8000](localhost:8000). There you can upload the json file that contains the details of the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect model performance (table)\n",
    "\n",
    "The performance of the models can also be viewed in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'regularization_rate': 0.000871406843381289, ...</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>189.750765</td>\n",
       "      <td>0.20</td>\n",
       "      <td>133.420563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'regularization_rate': 0.0006385919929132891,...</td>\n",
       "      <td>0.426667</td>\n",
       "      <td>1.935624</td>\n",
       "      <td>0.43</td>\n",
       "      <td>2.114420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model  train_acc  train_loss  \\\n",
       "0  {'regularization_rate': 0.000871406843381289, ...   0.360000  189.750765   \n",
       "1  {'regularization_rate': 0.0006385919929132891,...   0.426667    1.935624   \n",
       "\n",
       "   val_acc    val_loss  \n",
       "0     0.20  133.420563  \n",
       "1     0.43    2.114420  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelcomparisons = pd.DataFrame({'model':[str(params) for model, params, model_types in models],\n",
    "                       'train_acc': [history.history['acc'][-1] for history in histories],\n",
    "                       'train_loss': [history.history['loss'][-1] for history in histories],\n",
    "                       'val_acc': [history.history['val_acc'][-1] for history in histories],\n",
    "                       'val_loss': [history.history['val_loss'][-1] for history in histories]\n",
    "                       })\n",
    "modelcomparisons.to_csv(resultpath +'modelcomparisons.csv')\n",
    "\n",
    "modelcomparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Choose the best model\n",
    "Now that we found an effective architecture, we can choose the most promising model. For example, we can choose the model with the highest accuracy on the validation data set. To maximize this models performance, we will train this model on more data and more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type and parameters of the best model:\n",
      "DeepConvLSTM\n",
      "{'regularization_rate': 0.0006385919929132891, 'learning_rate': 0.0013206889428859167, 'lstm_dims': array([47]), 'filters': array([68, 44, 31, 62, 67])}\n"
     ]
    }
   ],
   "source": [
    "best_model_index = np.argmax(val_accuracies)\n",
    "best_model, best_params, best_model_types = models[best_model_index]\n",
    "print('Model type and parameters of the best model:')\n",
    "print(best_model_types)\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the best model on the full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have identified the best model architecture out of our random pool of models we can continue by training the model on the full training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15718 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "15718/15718 [==============================] - 10845s - loss: 1.1182 - acc: 0.7167 - val_loss: 0.4088 - val_acc: 0.9000\n"
     ]
    }
   ],
   "source": [
    "#We make a copy of the model, to start training from fresh\n",
    "nr_epochs = 1\n",
    "datasize = X_train.shape[0] #We're going to train the model on the complete data set\n",
    "history = best_model.fit(X_train[:datasize,:,:], y_train_binary[:datasize,:],\n",
    "              nb_epoch=nr_epochs, validation_data=(X_val, y_val_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the training process:\n",
    "find_architecture.plotTrainingProcess(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving, loading and comparing reloaded model with orignal model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modoel can be saved for future use. The savemodel function will save two separate files: a json file for the architecture and a npy (numpy array) file for the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelname = 'my_bestmodel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/PAMAP2/PAMAP2_Dataset/output/modelsmy_bestmodel_architecture.json',\n",
       " 'data/PAMAP2/PAMAP2_Dataset/output/modelsmy_bestmodel_weights')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage.savemodel(best_model,resultpath,modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dafne/anaconda2/envs/mcfly/lib/python3.5/site-packages/keras/engine/topology.py:368: UserWarning: The `regularizers` property of layers/models is deprecated. Regularization losses are now managed via the `losses` layer/model property.\n",
      "  warnings.warn('The `regularizers` property of '\n"
     ]
    }
   ],
   "source": [
    "model_reloaded = storage.loadmodel(resultpath,modelname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has been reloaded. Let's investigate whether it gives the same probability estimates as the original model in a small subset of the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 3s     \n",
      "10/10 [==============================] - 3s     \n"
     ]
    }
   ],
   "source": [
    "datasize = 10\n",
    "probs_original = best_model.predict_proba(X_val[:datasize,:,:],batch_size=1)\n",
    "probs_reloaded = model_reloaded.predict_proba(X_val[:datasize,:,:],batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(probs_reloaded == probs_original).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced model inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although beyond the scope of mcfly it may be worth highlighting that the objects 'models', 'best_model_fullytrained' and 'best_model' are Keras objects. This means that you can use Keras functions like .predict and .evaluate on the objects to run advanced analyses. These functions are all documented in the Keras documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 29s    \n"
     ]
    }
   ],
   "source": [
    "## Inspect model predictions\n",
    "datasize = X_val.shape[0]\n",
    "probs = best_model.predict_proba(X_val[:datasize,:,:],batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...,  0.          0.          0.01      ]\n",
      " [ 0.          0.01        0.         ...,  0.01        0.          0.04      ]\n",
      " [ 0.          0.          0.02       ...,  0.          0.          0.02      ]\n",
      " ..., \n",
      " [ 0.01        0.          0.01       ...,  0.          0.          0.02      ]\n",
      " [ 0.          0.57999998  0.02       ...,  0.01        0.06        0.01      ]\n",
      " [ 0.          0.01        0.         ...,  0.01        0.          0.03      ]]\n"
     ]
    }
   ],
   "source": [
    "print(np.round(probs,decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 323s   \n",
      "Score of best model: [0.51429932212829588, 0.86799999999999999]\n"
     ]
    }
   ],
   "source": [
    "## Test on Testset\n",
    "score_test = best_model.evaluate(X_test, y_test_binary, verbose=True)\n",
    "print('Score of best model: ' + str(score_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
